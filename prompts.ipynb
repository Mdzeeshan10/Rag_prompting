{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0be9afa6",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'venv' requires the ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: 'c:/test/venv/Scripts/python.exe -m pip install ipykernel -U --force-reinstall'"
     ]
    }
   ],
   "source": [
    "pip install PyPDF2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3eb9d4e9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import PyPDF2\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "db636ff5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#other method to check pdf data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8619b160",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4004cdd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d1bff98a",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'offset' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\mpasha1\\OneDrive - Insight\\Desktop\\work\\deployopenai\\final_PDF_to_Openai_Embeddings (1).ipynb Cell 6\u001b[0m line \u001b[0;36m1\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/mpasha1/OneDrive%20-%20Insight/Desktop/work/deployopenai/final_PDF_to_Openai_Embeddings%20%281%29.ipynb#X42sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m offset\n",
      "\u001b[1;31mNameError\u001b[0m: name 'offset' is not defined"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3c5b4cb7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8740a419",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2f3f373b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_pdf_to_txt(pdf_path, txt_path):\n",
    "    try:\n",
    "        with open(pdf_path, 'rb') as pdf_file:\n",
    "            pdf_reader = PyPDF2.PdfReader(pdf_file)\n",
    "            text = ''\n",
    "            for page in pdf_reader.pages:\n",
    "                text += page.extract_text()\n",
    "            \n",
    "            with open(txt_path, 'w', encoding='utf-8') as txt_file:\n",
    "                txt_file.write(text)\n",
    "            print(\"PDF converted to TXT successfully!\")\n",
    "    except Exception as e:\n",
    "        print(\"Error:\", e)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "33034577",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PDF converted to TXT successfully!\n"
     ]
    }
   ],
   "source": [
    "# Replace 'input.pdf' with your input PDF file path\n",
    "input_pdf_path = 'pdfdata/field-guide-to-data-science.pdf'\n",
    "\n",
    "# Replace 'output.txt' with the desired output TXT file path\n",
    "output_txt_path = 'txtdata/output.txt'\n",
    "\n",
    "convert_pdf_to_txt(input_pdf_path, output_txt_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ca99d5df",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b5f1e729",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Anti-Discrimination Policy.pdf',\n",
       " 'Blood Relation Policy.pdf',\n",
       " 'Career Progression - Roles & Definitions.pdf',\n",
       " 'Code of Conduct.pdf',\n",
       " 'Deputation Policy.pdf',\n",
       " 'Dress Code Policy.pdf',\n",
       " 'Electronic mail Policy.pdf',\n",
       " 'Expense - Quick Guide_Revised.pdf',\n",
       " 'GD Holiday Calendar 2023 (1).pdf',\n",
       " 'GD Secondary Billing Bonus Published.pdf',\n",
       " 'Grievance Redressal Policy.pdf',\n",
       " 'Hanu GMC POLICY BENEFITS 2022-23.pdf',\n",
       " 'HROne Quick Guide- For Manager.pdf',\n",
       " 'Induction Policy.pdf',\n",
       " 'Information Technology Policy.pdf',\n",
       " 'Internal Job Posting.pdf',\n",
       " 'Medical Insurance Policy.pdf',\n",
       " 'Mobile Phone Reimbursement Policy.pdf',\n",
       " 'Performance Appraisal.pdf',\n",
       " 'Performance Improvement Policy.pdf',\n",
       " 'Photo Identity Card Policy.pdf',\n",
       " 'PREVENTION OF SEXUAL HARASSMENT.pdf',\n",
       " 'Promotion Policy.pdf',\n",
       " 'Relationships at Work Policy.pdf',\n",
       " 'Relocation Policy.pdf',\n",
       " 'Safety & Hygiene Policy.pdf',\n",
       " 'training Policy.pdf',\n",
       " 'Travel Policy.pdf',\n",
       " 'Unified Delivery Progression Chart.pdf',\n",
       " 'Work Days Hours Policy.pdf']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.listdir('pdf_data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3747cc0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import PyPDF2\n",
    "\n",
    "def convert_pdf_to_txt(pdf_path, txt_path):\n",
    "    try:\n",
    "        with open(pdf_path, 'rb') as pdf_file:\n",
    "            pdf_reader = PyPDF2.PdfReader(pdf_file)\n",
    "            text = ''\n",
    "            for page in pdf_reader.pages:\n",
    "                text += page.extract_text()\n",
    "            \n",
    "            with open(txt_path, 'w', encoding='utf-8') as txt_file:\n",
    "                txt_file.write(text)\n",
    "            print(f\"PDF '{pdf_path}' converted to TXT successfully!\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error converting '{pdf_path}':\", e)\n",
    "\n",
    "def batch_convert_pdfs(input_folder, output_folder):\n",
    "    if not os.path.exists(output_folder):\n",
    "        os.makedirs(output_folder)\n",
    "    \n",
    "    for pdf_file in os.listdir(input_folder):\n",
    "        if pdf_file.endswith('.pdf'):\n",
    "            pdf_path = os.path.join(input_folder, pdf_file)\n",
    "            txt_filename = os.path.splitext(pdf_file)[0] + '.txt'\n",
    "            txt_path = os.path.join(output_folder, txt_filename)\n",
    "            convert_pdf_to_txt(pdf_path, txt_path)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a85eca30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PDF 'pdfdata\\azure-cognitive-services-openai.pdf' converted to TXT successfully!\n",
      "PDF 'pdfdata\\field-guide-to-data-science.pdf' converted to TXT successfully!\n",
      "PDF 'pdfdata\\GD Holiday Calendar 2023.pdf' converted to TXT successfully!\n"
     ]
    }
   ],
   "source": [
    "# Replace 'input_folder' with the folder containing your PDF files\n",
    "input_folder = 'pdfdata'\n",
    "\n",
    "# Replace 'output_folder' with the desired folder for the output TXT files\n",
    "output_folder = 'txtdata'\n",
    "\n",
    "batch_convert_pdfs(input_folder, output_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "65507049",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import openai\n",
    "# from dotenv import load_dotenv\n",
    "from langchain.chat_models import AzureChatOpenAI\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain.vectorstores import AzureSearch\n",
    "from langchain.document_loaders import DirectoryLoader\n",
    "from langchain.document_loaders import TextLoader\n",
    "from langchain.text_splitter import TokenTextSplitter\n",
    "from langchain.chains import ConversationalRetrievalChain\n",
    "from langchain.prompts import PromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1304d298",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load environment variables\n",
    "# load_dotenv()\n",
    "\n",
    "# Configure OpenAI API\n",
    "openai.api_type = \"azure\"\n",
    "openai.api_base = os.getenv('OPENAI_API_BASE')\n",
    "openai.api_key = os.getenv('OPENAI_API_KEY')\n",
    "openai.api_version = os.getenv('OPENAI_API_VERSION')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d66f6d8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load environment variables\n",
    "# load_dotenv()\n",
    "\n",
    "# Configure OpenAI API\n",
    "openai.api_type = \"azure\"\n",
    "openai.api_base = \"https://openai-test-005.openai.azure.com/\"    #os.getenv('OPENAI_API_BASE')\n",
    "openai.api_key =  \"411da2482a0945629107681bdb72740c\"  #os.getenv('OPENAI_API_KEY')\n",
    "openai.api_version = \"2023-07-01-preview\"   #os.getenv('OPENAI_API_VERSION')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "08d4ac9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\mpasha1\\AppData\\Local\\anaconda3\\lib\\site-packages\\langchain\\embeddings\\openai.py:214: UserWarning: WARNING! deployment_id is not default parameter.\n",
      "                    deployment_id was transferred to model_kwargs.\n",
      "                    Please confirm that deployment_id is what you intended.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Initialize gpt-35-turbo and our embedding model\n",
    "# llm = AzureChatOpenAI(deployment_name=\"gptnew\")\n",
    "# embeddings = OpenAIEmbeddings(deployment_id=\"textnew005\", chunk_size=1)\n",
    "\n",
    "# other wise\n",
    "\n",
    "llm = AzureChatOpenAI(\n",
    "    openai_api_key = \"223511cbbfe24088ab56663240096821\",\n",
    "    openai_api_base = \"https://openaizee.openai.azure.com/\",\n",
    "    openai_api_version = \"2023-07-01-preview\",\n",
    "    temperature=0,\n",
    "    deployment_name=\"gpt-35-trubo\")\n",
    "\n",
    "embeddings = OpenAIEmbeddings(\n",
    "    openai_api_key = \"223511cbbfe24088ab56663240096821\",\n",
    "    openai_api_base = \"https://openaizee.openai.azure.com/\",\n",
    "    openai_api_version = \"2023-07-01-preview\",\n",
    "    deployment_id=\"txtada\", chunk_size=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "399f31f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\mpasha1\\AppData\\Local\\anaconda3\\lib\\site-packages\\langchain\\embeddings\\openai.py:214: UserWarning: WARNING! deployment_id is not default parameter.\n",
      "                    deployment_id was transferred to model_kwargs.\n",
      "                    Please confirm that deployment_id is what you intended.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "llm = AzureChatOpenAI(\n",
    "    openai_api_key = \"411da2482a0945629107681bdb72740c\",\n",
    "\n",
    "    openai_api_base = \"https://openai-test-005.openai.azure.com/\",\n",
    "\n",
    "    openai_api_version = \"2023-07-01-preview\",\n",
    "    temperature=0,\n",
    "    \n",
    "    deployment_name=\"gptnew\")\n",
    "embeddings = OpenAIEmbeddings(\n",
    "    openai_api_key = \"411da2482a0945629107681bdb72740c\",\n",
    "\n",
    "    openai_api_base = \"https://openai-test-005.openai.azure.com/\",\n",
    "\n",
    "    openai_api_version = \"2023-07-01-preview\",\n",
    "    deployment_id=\"textnew005\", chunk_size=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2d24cbe0",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Endpoint must be a string.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\mpasha1\\AppData\\Local\\anaconda3\\lib\\site-packages\\azure\\search\\documents\\indexes\\_utils.py:74\u001b[0m, in \u001b[0;36mnormalize_endpoint\u001b[1;34m(endpoint)\u001b[0m\n\u001b[0;32m     73\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m---> 74\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m endpoint\u001b[39m.\u001b[39;49mlower()\u001b[39m.\u001b[39mstartswith(\u001b[39m\"\u001b[39m\u001b[39mhttp\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[0;32m     75\u001b[0m         endpoint \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mhttps://\u001b[39m\u001b[39m\"\u001b[39m \u001b[39m+\u001b[39m endpoint\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'lower'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\mpasha1\\OneDrive - Insight\\Desktop\\work\\deployopenai\\final_PDF_to_Openai_Embeddings (1).ipynb Cell 20\u001b[0m line \u001b[0;36m2\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/mpasha1/OneDrive%20-%20Insight/Desktop/work/deployopenai/final_PDF_to_Openai_Embeddings%20%281%29.ipynb#X16sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39m# Connect to Azure Cognitive Search\u001b[39;00m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/mpasha1/OneDrive%20-%20Insight/Desktop/work/deployopenai/final_PDF_to_Openai_Embeddings%20%281%29.ipynb#X16sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m acs \u001b[39m=\u001b[39m AzureSearch(azure_search_endpoint\u001b[39m=\u001b[39;49mos\u001b[39m.\u001b[39;49mgetenv(\u001b[39m'\u001b[39;49m\u001b[39mAZURE_COGNITIVE_SEARCH_SERVICE_NAME\u001b[39;49m\u001b[39m'\u001b[39;49m),\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/mpasha1/OneDrive%20-%20Insight/Desktop/work/deployopenai/final_PDF_to_Openai_Embeddings%20%281%29.ipynb#X16sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m                  azure_search_key\u001b[39m=\u001b[39;49mos\u001b[39m.\u001b[39;49mgetenv(\u001b[39m'\u001b[39;49m\u001b[39mAZURE_COGNITIVE_SEARCH_API_KEY\u001b[39;49m\u001b[39m'\u001b[39;49m),\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/mpasha1/OneDrive%20-%20Insight/Desktop/work/deployopenai/final_PDF_to_Openai_Embeddings%20%281%29.ipynb#X16sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m                  index_name\u001b[39m=\u001b[39;49mos\u001b[39m.\u001b[39;49mgetenv(\u001b[39m'\u001b[39;49m\u001b[39mAZURE_COGNITIVE_SEARCH_INDEX_NAME\u001b[39;49m\u001b[39m'\u001b[39;49m),\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/mpasha1/OneDrive%20-%20Insight/Desktop/work/deployopenai/final_PDF_to_Openai_Embeddings%20%281%29.ipynb#X16sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m                  embedding_function\u001b[39m=\u001b[39;49membeddings\u001b[39m.\u001b[39;49membed_query)\n",
      "File \u001b[1;32mc:\\Users\\mpasha1\\AppData\\Local\\anaconda3\\lib\\site-packages\\langchain\\vectorstores\\azuresearch.py:234\u001b[0m, in \u001b[0;36mAzureSearch.__init__\u001b[1;34m(self, azure_search_endpoint, azure_search_key, index_name, embedding_function, search_type, semantic_configuration_name, semantic_query_language, fields, vector_search, semantic_settings, scoring_profiles, default_scoring_profile, **kwargs)\u001b[0m\n\u001b[0;32m    232\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39muser_agent\u001b[39m\u001b[39m\"\u001b[39m \u001b[39min\u001b[39;00m kwargs \u001b[39mand\u001b[39;00m kwargs[\u001b[39m\"\u001b[39m\u001b[39muser_agent\u001b[39m\u001b[39m\"\u001b[39m]:\n\u001b[0;32m    233\u001b[0m     user_agent \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m \u001b[39m\u001b[39m\"\u001b[39m \u001b[39m+\u001b[39m kwargs[\u001b[39m\"\u001b[39m\u001b[39muser_agent\u001b[39m\u001b[39m\"\u001b[39m]\n\u001b[1;32m--> 234\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mclient \u001b[39m=\u001b[39m _get_search_client(\n\u001b[0;32m    235\u001b[0m     azure_search_endpoint,\n\u001b[0;32m    236\u001b[0m     azure_search_key,\n\u001b[0;32m    237\u001b[0m     index_name,\n\u001b[0;32m    238\u001b[0m     semantic_configuration_name\u001b[39m=\u001b[39;49msemantic_configuration_name,\n\u001b[0;32m    239\u001b[0m     fields\u001b[39m=\u001b[39;49mfields,\n\u001b[0;32m    240\u001b[0m     vector_search\u001b[39m=\u001b[39;49mvector_search,\n\u001b[0;32m    241\u001b[0m     semantic_settings\u001b[39m=\u001b[39;49msemantic_settings,\n\u001b[0;32m    242\u001b[0m     scoring_profiles\u001b[39m=\u001b[39;49mscoring_profiles,\n\u001b[0;32m    243\u001b[0m     default_scoring_profile\u001b[39m=\u001b[39;49mdefault_scoring_profile,\n\u001b[0;32m    244\u001b[0m     default_fields\u001b[39m=\u001b[39;49mdefault_fields,\n\u001b[0;32m    245\u001b[0m     user_agent\u001b[39m=\u001b[39;49muser_agent,\n\u001b[0;32m    246\u001b[0m )\n\u001b[0;32m    247\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msearch_type \u001b[39m=\u001b[39m search_type\n\u001b[0;32m    248\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msemantic_configuration_name \u001b[39m=\u001b[39m semantic_configuration_name\n",
      "File \u001b[1;32mc:\\Users\\mpasha1\\AppData\\Local\\anaconda3\\lib\\site-packages\\langchain\\vectorstores\\azuresearch.py:98\u001b[0m, in \u001b[0;36m_get_search_client\u001b[1;34m(endpoint, key, index_name, semantic_configuration_name, fields, vector_search, semantic_settings, scoring_profiles, default_scoring_profile, default_fields, user_agent)\u001b[0m\n\u001b[0;32m     96\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m     97\u001b[0m     credential \u001b[39m=\u001b[39m AzureKeyCredential(key)\n\u001b[1;32m---> 98\u001b[0m index_client: SearchIndexClient \u001b[39m=\u001b[39m SearchIndexClient(\n\u001b[0;32m     99\u001b[0m     endpoint\u001b[39m=\u001b[39;49mendpoint, credential\u001b[39m=\u001b[39;49mcredential, user_agent\u001b[39m=\u001b[39;49muser_agent\n\u001b[0;32m    100\u001b[0m )\n\u001b[0;32m    101\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m    102\u001b[0m     index_client\u001b[39m.\u001b[39mget_index(name\u001b[39m=\u001b[39mindex_name)\n",
      "File \u001b[1;32mc:\\Users\\mpasha1\\AppData\\Local\\anaconda3\\lib\\site-packages\\azure\\search\\documents\\indexes\\_search_index_client.py:43\u001b[0m, in \u001b[0;36mSearchIndexClient.__init__\u001b[1;34m(self, endpoint, credential, **kwargs)\u001b[0m\n\u001b[0;32m     41\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\u001b[39mself\u001b[39m, endpoint: \u001b[39mstr\u001b[39m, credential: Union[AzureKeyCredential, TokenCredential], \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs: Any) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m     42\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_api_version \u001b[39m=\u001b[39m kwargs\u001b[39m.\u001b[39mpop(\u001b[39m\"\u001b[39m\u001b[39mapi_version\u001b[39m\u001b[39m\"\u001b[39m, DEFAULT_VERSION)\n\u001b[1;32m---> 43\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_endpoint \u001b[39m=\u001b[39m normalize_endpoint(endpoint)\n\u001b[0;32m     44\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_credential \u001b[39m=\u001b[39m credential\n\u001b[0;32m     45\u001b[0m     audience \u001b[39m=\u001b[39m kwargs\u001b[39m.\u001b[39mpop(\u001b[39m\"\u001b[39m\u001b[39maudience\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mNone\u001b[39;00m)\n",
      "File \u001b[1;32mc:\\Users\\mpasha1\\AppData\\Local\\anaconda3\\lib\\site-packages\\azure\\search\\documents\\indexes\\_utils.py:80\u001b[0m, in \u001b[0;36mnormalize_endpoint\u001b[1;34m(endpoint)\u001b[0m\n\u001b[0;32m     78\u001b[0m     \u001b[39mreturn\u001b[39;00m endpoint\n\u001b[0;32m     79\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mAttributeError\u001b[39;00m \u001b[39mas\u001b[39;00m ex:\n\u001b[1;32m---> 80\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mEndpoint must be a string.\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39mfrom\u001b[39;00m \u001b[39mex\u001b[39;00m\n",
      "\u001b[1;31mValueError\u001b[0m: Endpoint must be a string."
     ]
    }
   ],
   "source": [
    "# Connect to Azure Cognitive Search\n",
    "acs = AzureSearch(azure_search_endpoint=os.getenv('AZURE_COGNITIVE_SEARCH_SERVICE_NAME'),\n",
    "                 azure_search_key=os.getenv('AZURE_COGNITIVE_SEARCH_API_KEY'),\n",
    "                 index_name=os.getenv('AZURE_COGNITIVE_SEARCH_INDEX_NAME'),\n",
    "                 embedding_function=embeddings.embed_query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e8e353fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connect to Azure Cognitive Search ankur cridential\n",
    "acs = AzureSearch(azure_search_endpoint=\"https://cog-sr-001.search.windows.net\",\n",
    "                 azure_search_key=\"eydDYpgoKUnW4J3xMt04jHYW1k31JmoVms7rIK3WSkAzSeBRJ4jk\",\n",
    "                 index_name=\"zee4\",\n",
    "                 embedding_function=embeddings.embed_query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7b40ab93",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ZTViMGI1YzQtOGYyYi00MmU2LTk4Y2ItMzczZWRmN2YyZDM0',\n",
       " 'OTRkMTc2ZWQtYWUzYy00Zjg4LTg0MjItMDhmMjM0ZWFmMzll',\n",
       " 'MDdhZTZmOGQtYzQ0OC00MWM0LWJhZmItN2UyMmU4NjQyZWMy',\n",
       " 'ZTA5YzJiN2EtMjRhNC00M2ZkLTg2NDMtYzgxNzViNzcwZDcw',\n",
       " 'ZmFhNTM3YmQtZWRlZS00NDBmLTgxNjktZDdmMWY1Y2Y5MzE5',\n",
       " 'ZjFjMzA3YTUtNDlkZC00MWRhLTkzMjEtN2UzMjllMGY5M2I3',\n",
       " 'YmRiZDRkOTgtNDlmMi00MWZmLWE2ZmQtYzU0NDJkYTJhMjRk',\n",
       " 'YjcwNTE2YTEtNmRiZi00ZjQzLWE3MzUtMzZhMTVkNzMyNzJk',\n",
       " 'NzY5NjE4MTMtZWRiZi00MzRlLTlhMjktNTAwZDM0MjY0ZjYx',\n",
       " 'NzYyMDA1ZmQtMzc2Yi00YmJlLWEwN2ItNzBkNmJhNWMxYTdl',\n",
       " 'NjAxNDRmNGYtMzZiOS00YmU4LWFjYWMtMjI1N2U5MGVhNWRk',\n",
       " 'YmM4N2U1MWYtNGMwMi00ZWI3LThjYjItY2JhMGYwOTQ5Mzcy',\n",
       " 'ODE2OWY4ZjUtYjgzYi00NmNjLTkyMjEtMjhlNDQ4ZjU4MzRl',\n",
       " 'NDYzNDU4ZGYtMDU2MS00OGJmLTg0NzctM2QwMTliOGNlZjUz',\n",
       " 'MTI2MzQ1NmUtMjg2YS00ZTRiLWE5NDAtY2I0YjA3NTRjYThk',\n",
       " 'ZDJkYTAzN2MtZDBhMy00MjcxLWJmNGMtMTJjMDUwNmVkZTc5',\n",
       " 'MmQ0Nzg0ZTktZjE0YS00OTMyLWI0YTEtNzljNzkzZmFkZmZl',\n",
       " 'ZjVkZTE1ZjAtNGQ5ZS00YzdkLTg3NGItNmRlZTI2OGQ4Mzg5',\n",
       " 'NGVlMWI5ZWMtNzVlYi00OWMzLTkxMTctMjEwNWU5NWQ0NDY4',\n",
       " 'MzlkODE2MmYtOWJhOC00ZGQ5LTg3MDEtYzk2NGEzYmI4NDM1',\n",
       " 'Mjk5NDlkZWEtODliMS00NmZlLTgyNTItNDAxZWU1Y2QzMmVm',\n",
       " 'NzZkOTZlOTctNzM5NS00MDA0LTk4M2QtNjM1NjljNGQyOTRi',\n",
       " 'MTM4MDA3YmQtZGJmZi00ZmI2LWJkMDAtYmQxN2RmZTc4Njlm',\n",
       " 'YjJmM2UwMGYtMDc4OS00NmRhLWI4Y2EtNDRlMTcxMzhhOGQ1',\n",
       " 'N2M2ZGQ4NGItYmQzNi00MjcyLWJkNGMtZTM5NWYwZGZjNjE2',\n",
       " 'NDFmYmQ0MzItYTZhNC00NGYwLTkxMjEtOTZiZmRkNzE0NDAz',\n",
       " 'NGE3ODdkODItMDhjZS00OTZlLTkwZTEtMGU3ZjZkZDY4MjFk',\n",
       " 'YzZiMGNlNTgtZTFlOS00MTBlLTk0OWItYTcyZDZjMjRlYzM4',\n",
       " 'MWRkYmQwOGQtYTcwYS00ODY3LWExMDYtY2E5ZmM0MTRhNmIx',\n",
       " 'MTZhZGIyNjItOWU2Ni00ZGJkLWFmODMtN2M3M2Q2NTlmMDQ2',\n",
       " 'NDQxOTcwMzItNzRiYi00ODExLWI2MzQtMzU5ZmY4MjdhNTU3',\n",
       " 'OThiZGEwNmItZmY5Yi00ODAyLTgxMDctMzEyMTFlMTQ1NmQ2',\n",
       " 'NTBkZmI0YzUtZDAwMi00ZDcwLTgyYWUtMmM3MjE4ODc2NWJi',\n",
       " 'OTJmZDc3OWUtNWIzOC00M2IyLWFiODEtZDA0NDcxMTM4YzA0',\n",
       " 'Y2I3ODljZjYtMTZjZC00MzlmLTk5NjItZTAyZGQ0NjJjMjM4',\n",
       " 'MjZmNDJiMjMtZWZlZC00YzZmLWJhMWUtMmJkNWU1Njk4YWIy',\n",
       " 'ZjY2YmQ4OGYtNDhlNy00M2JhLWEzYzYtZjY0ZWM5M2U5NzRm',\n",
       " 'YzU3ZWZlNjktNDA4OC00M2VhLWE5MTgtYzNiZDQ2ODlkYjll',\n",
       " 'OTgzNzNkNTktZTMzOC00M2FiLWFiNDAtMWUyNWQ0NDUzN2Iz',\n",
       " 'YWY1OWJiMTEtNGM3ZS00ZTBlLWEwMDAtMDhkNWRlZjc3NTVi',\n",
       " 'NjJlNThhNDktOThhNC00MmM5LTg5NmYtNDEwYzZjOGY3NDUy',\n",
       " 'M2E0OTZiYTctNmZmNS00YmEwLTkxMmUtOWFjYTZiNTE4MDY0',\n",
       " 'ZGI0ZDM5YmYtMTY4YS00M2Q5LWE2YjktYTQ0OTRkZDA1ZGY0',\n",
       " 'MmFiMzA4YzktZGZiMi00ZGE3LTk0ZWEtNTZkNTU0YmM0OTVm',\n",
       " 'NDZhYzNmZmItYjhlYS00MjViLTg0NGItYmEyZGIxODAxMjZm',\n",
       " 'Y2JjNjUwNTQtOTMyZC00ZjE1LWE0MTUtNTBmOGRhNzZiMWZh',\n",
       " 'MDUyODFjMWMtM2Y3Mi00MGJjLWFhODctMWMwMjdhYzZiZGNk',\n",
       " 'MmIwMzJhMjAtOWM2Yi00NWYzLTkwNzctMmViZThlMjY1Yzc0',\n",
       " 'ZDdjM2Q4MDAtM2E3MC00NTNiLTgyMzItNzAzYTc2ZWU0NDY1',\n",
       " 'ZTg2OTY2OGQtODAzMi00NDRjLWFkMzEtZmU3NGE2YzM5N2Jk',\n",
       " 'YmYxYTc3NDItMDhhNC00OTIyLTg1MTMtYjIxMDU0ZWQxNzEy',\n",
       " 'ZmU2ZTE1MDMtZmM4YS00ZWU0LTk3YzgtZDE0N2M3YjQ1ZGUy']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loader = DirectoryLoader('txtdata/', glob=\"*.txt\", loader_cls=TextLoader, loader_kwargs={'autodetect_encoding': True})\n",
    "\n",
    "documents = loader.load()\n",
    "text_splitter = TokenTextSplitter(chunk_size=1000, chunk_overlap=0)\n",
    "docs = text_splitter.split_documents(documents)\n",
    "\n",
    "# Add documents to Azure Search\n",
    "acs.add_documents(documents=docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "9319a9a6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "7e6e8d2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "isticated methods such as support vector machines, but the limit \n",
      "still exists nonetheless.\n",
      "A counterintuitive consequence of the curse of dimensionality is  \n",
      "that it limits the amount of data needed to train a classification  \n",
      "model. There are roughly two reasons for this phenomenon. In one \n",
      "case, the dimensionality is small enough that the model can be  \n",
      "trained on a single machine. In the other case, the exponentially \n",
      "expanding complexity of a high-dimensionality problem makes it \n",
      "(practically) computationally impossible to train a model. In our \n",
      "experience, it is quite rare for a problem to fall in a “sweet spot” \n",
      "between these two extremes. \n",
      "Rather than trying to create super-scalable algorithm \n",
      "implementations, focus your attention on solving your immediate \n",
      "problems with basic methods. Wait until you encounter a problem \n",
      "where an algorithm fails to converge or provides poor cross-validated \n",
      "results, and then seek new approaches. Only when you find that \n",
      "alternate approaches don’t already exist, should you begin building \n",
      "new implementations. The expected cost of this work pattern is lower \n",
      "than over-engineering right out of the gate. \n",
      "Put otherwise, “Keep it simple, stupid”.\n",
      "THE FIELD  GUIDE to  DATA  SCIENCEBaking the Cake \n",
      "I was once given a time series set of roughly \n",
      "1,600 predictor variables and 16 target variables \n",
      "and asked to implement a number of modeling \n",
      "techniques to predict the target variable \n",
      "values. The client was challenged to handle the \n",
      "complexity associated with the large number of \n",
      "variables and needed help. Not only did I have \n",
      "a case of the curse, but the predictor variables \n",
      "were also quite diverse. At ﬁrst glance, it looked \n",
      "like trying to bake a cake with everything in the cupboard.  \n",
      "That is not a good way to bake or to make predictions! \n",
      "The data diversity could be \n",
      "partially explained by the fact \n",
      "that the time series predictors \n",
      "did not all have the same \n",
      "periodicity. The target time \n",
      "series were all daily values \n",
      "whereas the predictors were \n",
      "daily, weekly, quarterly, and \n",
      "monthly. This was tricky to \n",
      "sort out, given that imputing \n",
      "zeros isn’t likely to produce \n",
      "good results. For this speciﬁc \n",
      "reason, I chose to use neural \n",
      "networks for evaluating the \n",
      "weekly variable contributions. Using this approach, I was \n",
      "able to condition upon week, \n",
      "without greatly increasing \n",
      "the dimensionality. For the \n",
      "other predictors, I used \n",
      "a variety of techniques, \n",
      "including projection and \n",
      "correlation, to make heads \n",
      "or tails of the predictors. My \n",
      "approach successfully reduced \n",
      "the number of variables, \n",
      "accomplishing the client’s goal \n",
      "of making the problem space \n",
      "tractable. As a result, the cake \n",
      "turned out just ﬁne.››\n",
      "Stephanie \n",
      "Rivera\n",
      "99 Life in the TrenchesModel Validation\n",
      "Repeating what you just heard does not mean that you  \n",
      "learned anything. \n",
      "Model validation is central to construction of any model. This answers \n",
      "the question “How well did my hypothesis fit the observed data?”\n",
      "If we do not have enough data, our models cannot connect the dots. \n",
      "On the other hand, given too much data the model cannot think \n",
      "outside of the box. The model learns specific details about the training \n",
      "data that do not generalize to the population. This is the problem of \n",
      "model over fitting. \n",
      "Many techniques exist to combat model over fitting. The simplest \n",
      "method is to split your dataset into training, testing and validation \n",
      "sets. The training data is used to construct the model. The model \n",
      "constructed with the training data is then evaluated with the testing \n",
      "data. The performance of the model against the testing set is used to \n",
      "further reduce model error. This indirectly includes the testing data \n",
      "within model construction, helping to reduce model over fit. Finally, \n",
      "the model is evaluated on the validation data to assess how well the \n",
      "model generalizes.\n",
      "A few methods where the data is split into training and testing sets \n",
      "include: k-fold cross-validation, Leave-One-Out cross-validation, \n",
      "bootstrap methods, and resampling methods. Leave-One-Out cross-\n",
      "valid\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "684bad8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "acs = AzureSearch(azure_search_endpoint=\"https://zeeshan-cog.search.windows.net\",\n",
    "\n",
    "                 azure_search_key=\"Z71ALe8TrNbMmOT0S2B5u7jX0DiOPKsiIwTsD5135YAzSeAHqFpG\",\n",
    "\n",
    "                 index_name=\"zee4\",\n",
    "\n",
    "                 embedding_function=embeddings.embed_query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0327c7b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   \n",
      " \n",
      "Note:  \n",
      "1. The total number of Holidays are restricted to 14 days including 2 RHs (Restricted Holidays) . \n",
      "2. Holidays falling on Saturday will be observed on Friday  and H olidays falling on Sunday will be  observed on Monday.   \n",
      " \n",
      " GD Holiday Calendar 2023  \n",
      "  \n",
      ". \n",
      " Holiday List (G.H.)  \n",
      "Holiday Date  Day \n",
      "New Year's Day  2 January 202 3 Monday  \n",
      "Republic Day  26 January 202 3 Thursday  \n",
      "Holi 8 March 202 3 Wednesday  \n",
      "Good Friday/ Tamil New Year/ Mesadi  7 April 202 3 Friday  \n",
      "Eid al -Adha  29 June 202 3 Thursday  \n",
      "Independence Day  15 August 202 3 Tuesday  \n",
      "Raksha Bandhan  30 August 202 3 Wednesday  \n",
      "Mahatma Gandhi Jayanti  2 October 202 3 Monday  \n",
      "Dussehra/Ayudha Pooja  24 October 202 3 Tuesday  \n",
      "Diwali/Goverdhan Puja  13 November 202 3 Monday  \n",
      "Guru Nanak's Birthday  27 November 202 3 Monday  \n",
      "Christmas  25 December 202 3 Monday  \n",
      "RH 2023 (For Other Dep artments )  \n",
      "RH 2023 (For HRO Dep artment ) \n",
      "Holiday  Date  Day  Holiday  Date  Day \n",
      "Makar Sankranti  13 January 202 3 Friday   \n",
      "Makar Sankranti  13 January 202 3 Friday  \n",
      "Pongal/Marten \n",
      "Luther King Day  16 January 202 3 Monday   \n",
      "Pongal/Marten \n",
      "Luther King Day  16 January 202 3 Monday  \n",
      "Ugadi/Gudi Padwa  22 March 202 3 Wednesday   \n",
      "Maha \n",
      "Shivaratri/Shivaratri \n",
      "(HRO only ) 18 February 202 3 Saturday  \n",
      "Vaisakhi  14 April 202 3 Friday   \n",
      "Ugadi/Gudi Padwa  22 March 202 3 Wednesday  \n",
      "May Day/ \n",
      "Maharashtra  \n",
      "Day/Gujarat day  01 May 202 3 Monday   \n",
      "Vaisakhi  14 April 202 3 Friday  \n",
      "Memorial Day (US)  29 May 202 3 Monday   \n",
      "Ramzan Id/Eid -ul-\n",
      "Fitar (HRO  only ) 22 April 202 3 Saturday  \n",
      "Independence Day \n",
      "(US)  04 July 202 3 Tuesday   \n",
      "Memorial Day (US)  29 May 202 3 Monday  \n",
      "Onam  29 August 202 3 Tuesday   \n",
      "Independence Day \n",
      "(US)  04 July 202 3 Tuesday  \n",
      "Labour Day (US)  04 September 202 3 Monday   \n",
      "Muharram/Ashura  \n",
      " (HRO  only ) 29 July 202 3 Saturday  \n",
      "Janmashtami/Krishn\n",
      "a Jayanti  07 September 202 3 Thursday   \n",
      "Onam  29 August 202 3 Tuesday  \n",
      "Ganesh \n",
      "Chaturthi/Vinayaka \n",
      "Chaturthi  19 September 202 3 Tuesday   \n",
      "Labour Day (US)  04 September 202 3 Monday  \n",
      "Bhai Duj  15 November 202 3 Wednesday   \n",
      "Janmashtami/Krish\n",
      "na Jayanti  07 September 202 3 Thursday  \n",
      "Thanksgiving Day \n",
      "(US)  23 November 202 3 Thursday   \n",
      "Ganesh \n",
      "Chaturthi/Vinayaka \n",
      "Chaturthi  19 September 202 3 Tuesday  \n",
      "The Friday after \n",
      "Thanksgiving Day \n",
      "(US)  24 November 202 3 Friday   \n",
      "Bhai Duj  15 November 202 3 Wednesday  \n",
      "Christmas Eve  22 December 202 3 Friday   \n",
      "Christmas Eve  22 December 202 3 Friday  \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Perform a similarity search\n",
    "docs = acs.similarity_search(query=\"leave Holiday\",\n",
    "# similarity_search_with_relevance_scores(query=\"leave dates\",k=4, score_threshold=0.80)\n",
    "\n",
    "\n",
    "    k=3,\n",
    "    search_type=\"similarity\",\n",
    ")\n",
    "# from pprint import pprint\n",
    "print(docs[0].page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "74efab78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chat roles\n",
    "SYSTEM = \"system\"\n",
    "USER = \"user\"\n",
    "ASSISTANT = \"assistant\"\n",
    "\n",
    "system_message_chat_conversation = \"\"\"Assistant helps the company employees with their Policy plan questions, and questions about the employee handbook. Be brief in your answers.\n",
    "Answer ONLY with the facts listed in the list of sources below. If there isn't enough information below, say you don't know. Do not generate answers that don't use the sources below. If asking a clarifying question to the user would help, ask the question.\n",
    "Each source has a name followed by colon and the actual information, always include the source name for each fact you use in the response. Use square brackets to reference the source, e.g. [info1.txt]. Don't combine sources, list each source separately, e.g. [info1.txt][info2.pdf].\n",
    "\"\"\"\n",
    "chat_conversations = [{\"role\" : SYSTEM, \"content\" : system_message_chat_conversation}]\n",
    "\n",
    "summary_prompt_template = \"\"\"\n",
    "Given the chat history and user question generate a search query that will return the best answer from the knowledge base.\n",
    "Try and generate a grammatical sentence for the search query.\n",
    "Do NOT use quotes and avoid other search operators.\n",
    "Do not include cited source filenames and document names such as info.txt or doc.pdf in the search query terms.\n",
    "Do not include any text inside [] or <<>> in the search query terms.\n",
    "If the question is not in English, translate the question to English before generating the search query.\n",
    "\n",
    "Search query:\n",
    "\"\"\"\n",
    "query_summary_conversations = {\"role\" : SYSTEM, \"content\" : summary_prompt_template}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d86624d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adapt if needed\n",
    "CONDENSE_QUESTION_PROMPT = PromptTemplate.from_template(\"\"\"\n",
    "SYSTEM = \"system\"\n",
    "USER = \"user\"\n",
    "ASSISTANT = \"assistant\"\n",
    "\n",
    "system_message_chat_conversation = \"\"\"Assistant helps the company employees with their Policy plan questions, and questions about the employee handbook. Be brief in your answers.\n",
    "Answer ONLY with the facts listed in the list of sources below. If there isn't enough information below, say you don't know. Do not generate answers that don't use the sources below. If asking a clarifying question to the user would help, ask the question.\n",
    "Each source has a name followed by colon and the actual information, always include the source name for each fact you use in the response. Use square brackets to reference the source, e.g. [info1.txt]. Don't combine sources, list each source separately, e.g. [info1.txt][info2.pdf].\n",
    "\"\"\"\n",
    "chat_conversations = [{\"role\" : SYSTEM, \"content\" : system_message_chat_conversation}]\n",
    "                                                        \n",
    "                                                        \n",
    "                                                        Given the chat history and user question generate a search query that will return the best answer from the knowledge base.\n",
    "Try and generate a grammatical sentence for the search query.\n",
    "Do NOT use quotes and avoid other search operators.\n",
    "Do not include cited source filenames and document names such as info.txt or doc.pdf in the search query terms.\n",
    "Do not include any text inside [] or <<>> in the search query terms.\n",
    "If the question is not in English, translate the question to English before generating the search query.\n",
    "\n",
    "Search query:\n",
    "                                                        \n",
    "Chat History:\n",
    "{chat_history}\n",
    "Follow Up Input: {question}\n",
    "Standalone question:\"\"\")\n",
    "\n",
    "qa = ConversationalRetrievalChain.from_llm(llm=llm,\n",
    "                                           retriever=acs.as_retriever(),\n",
    "                                           condense_question_prompt=CONDENSE_QUESTION_PROMPT,\n",
    "                                           return_source_documents=True,\n",
    "                                           verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "9019e1cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adapt if needed\n",
    "CONDENSE_QUESTION_PROMPT = PromptTemplate.from_template(\"\"\"\n",
    "                                                        \n",
    "Assistant helps the company employees with their Policy plan questions, and questions about the employee handbook. Be brief in your answers.\n",
    "Answer ONLY with the facts listed in the list of sources below. If there isn't enough information below, say you don't know. Do not generate answers that don't use the sources below. If asking a clarifying question to the user would help, ask the question.\n",
    "Each source has a name followed by colon and the actual information, always include the source name for each fact you use in the response. Use square brackets to reference the source, e.g. [info1.txt]. Don't combine sources, list each source separately, e.g. [info1.txt][info2.pdf]\n",
    "\n",
    "Given the chat history and user question generate a search query that will return the best answer from the knowledge base.\n",
    "Try and generate a grammatical sentence for the search query.\n",
    "Do NOT use quotes and avoid other search operators.\n",
    "Do not include cited source filenames and document names such as info.txt or doc.pdf in the search query terms.\n",
    "Do not include any text inside [] or <<>> in the search query terms.\n",
    "If the question is not in English, translate the question to English before generating the search query.\n",
    "\n",
    "                                                        \n",
    "Search query:\n",
    "                                                        \n",
    "\n",
    "Chat History:\n",
    "{chat_history}\n",
    "Follow Up Input: {question}\n",
    "Standalone question:\"\"\")\n",
    "\n",
    "qa = ConversationalRetrievalChain.from_llm(llm=llm,\n",
    "                                           retriever=acs.as_retriever(),\n",
    "                                           condense_question_prompt=CONDENSE_QUESTION_PROMPT,\n",
    "                                           return_source_documents=True,\n",
    "                                           verbose=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d916ea2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: What makes Data Science Diferent??\n",
      "Answer: Data Science is different from other fields because it involves the use of statistical and computational methods to extract insights and knowledge from data. It combines elements of statistics, mathematics, computer science, and domain expertise to analyze and interpret complex data sets. Data Science also involves the use of machine learning algorithms and artificial intelligence techniques to build predictive models and make data-driven decisions. Additionally, Data Science requires a strong understanding of data preparation and cleaning, as well as the ability to communicate findings to both technical and non-technical audiences.\n"
     ]
    }
   ],
   "source": [
    "chat_history = []\n",
    "query = \"What makes Data Science Diferent??\"\n",
    "result = qa({\"question\": query, \"chat_history\": chat_history})\n",
    "\n",
    "print(\"Question:\", query)\n",
    "print(\"Answer:\", result[\"answer\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "6e8d6cc2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: performance appraisalin hanu kya hai\n",
      "Answer: Hanu Software Solutions India Pvt. Ltd. has a Performance Appraisal Policy that defines the guidelines and due processes for Rockstars (employees) to undergo performance evaluation and appraisal. The Performance Appraisal Cycle spreads from January to December, and it consists of Rockstars' reviews of goals and a salary revision exercise. Reviews are done quarterly, and salary revision is taken once a year. The policy covers all Rockstars of Hanu Software Solutions India across different departments and Pan India geographies.\n"
     ]
    }
   ],
   "source": [
    "chat_history = []\n",
    "query = \"performance appraisalin hanu kya hai\"\n",
    "result = qa({\"question\": query, \"chat_history\": chat_history})\n",
    "\n",
    "print(\"Question:\", query)\n",
    "print(\"Answer:\", result[\"answer\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "4e9c8a9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: give me holiday list GD Holiday Calendar 2023 reference?\n",
      "Answer: Yes, here is the GD Holiday Calendar 2023 in table format:\n",
      "\n",
      "| Holiday | Date | Day |\n",
      "|---------|------|-----|\n",
      "| New Year's Day | 2 January 2023 | Monday |\n",
      "| Republic Day | 26 January 2023 | Thursday |\n",
      "| Holi | 8 March 2023 | Wednesday |\n",
      "| Good Friday/ Tamil New Year/ Mesadi | 7 April 2023 | Friday |\n",
      "| Eid al-Adha | 29 June 2023 | Thursday |\n",
      "| Independence Day | 15 August 2023 | Tuesday |\n",
      "| Raksha Bandhan | 30 August 2023 | Wednesday |\n",
      "| Mahatma Gandhi Jayanti | 2 October 2023 | Monday |\n",
      "| Dussehra/Ayudha Pooja | 24 October 2023 | Tuesday |\n",
      "| Diwali/Goverdhan Puja | 13 November 2023 | Monday |\n",
      "| Guru Nanak's Birthday | 27 November 2023 | Monday |\n",
      "| Christmas | 25 December 2023 | Monday | \n",
      "\n",
      "Note: The total number of Holidays are restricted to 14 days including 2 RHs (Restricted Holidays). Holidays falling on Saturday will be observed on Friday and Holidays falling on Sunday will be observed on Monday.\n"
     ]
    }
   ],
   "source": [
    "chat_history = [(query, result[\"answer\"])]\n",
    "query = \"give me holiday list GD Holiday Calendar 2023 reference?\"\n",
    "result = qa({\"question\": query, \"chat_history\": chat_history})\n",
    "\n",
    "print(\"Question:\", query)\n",
    "print(\"Answer:\", result[\"answer\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "a5e4a772",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: give me holiday list GD Holiday Calendar 2023 reference?provide me as a table format\n",
      "Answer: Yes, here is the GD Holiday Calendar for 2023 in a table format:\n",
      "\n",
      "| Holiday | Date | Day |\n",
      "|---------|------|-----|\n",
      "| New Year's Day | 2 January 2023 | Monday |\n",
      "| Republic Day | 26 January 2023 | Thursday |\n",
      "| Holi | 8 March 2023 | Wednesday |\n",
      "| Good Friday/ Tamil New Year/ Mesadi | 7 April 2023 | Friday |\n",
      "| Eid al-Adha | 29 June 2023 | Thursday |\n",
      "| Independence Day | 15 August 2023 | Tuesday |\n",
      "| Raksha Bandhan | 30 August 2023 | Wednesday |\n",
      "| Mahatma Gandhi Jayanti | 2 October 2023 | Monday |\n",
      "| Dussehra/Ayudha Pooja | 24 October 2023 | Tuesday |\n",
      "| Diwali/Goverdhan Puja | 13 November 2023 | Monday |\n",
      "| Guru Nanak's Birthday | 27 November 2023 | Monday |\n",
      "| Christmas | 25 December 2023 | Monday | \n",
      "\n",
      "Note: The total number of Holidays are restricted to 14 days including 2 RHs (Restricted Holidays). Holidays falling on Saturday will be observed on Friday and Holidays falling on Sunday will be observed on Monday.\n"
     ]
    }
   ],
   "source": [
    "chat_history = [(query, result[\"answer\"])]\n",
    "query = \"give me holiday list GD Holiday Calendar 2023 reference?provide me as a table format\"\n",
    "result = qa({\"question\": query, \"chat_history\": chat_history})\n",
    "\n",
    "print(\"Question:\", query)\n",
    "print(\"Answer:\", result[\"answer\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc3a4198",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PDF LOADER in Langchain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pypdf\n",
      "  Downloading pypdf-3.17.1-py3-none-any.whl (277 kB)\n",
      "     -------------------------------------- 277.6/277.6 kB 3.4 MB/s eta 0:00:00\n",
      "Installing collected packages: pypdf\n",
      "Successfully installed pypdf-3.17.1\n"
     ]
    }
   ],
   "source": [
    "! pip install pypdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import PyPDFLoader\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "import pypdf\n",
    "loader = PyPDFLoader(\"C:/Users/mpasha1/OneDrive - Insight/Desktop/work/deployopenai/pdf_data/GD Holiday Calendar 2023 (1).pdf\")\n",
    "text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=10)\n",
    "pages = loader.load_and_split(text_splitter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content=\"Note:  \\n1. The total number of Holidays are restricted to 14 days including 2 RHs (Restricted Holidays) . \\n2. Holidays falling on Saturday will be observed on Friday  and H olidays falling on Sunday will be  observed on Monday.   \\n \\n GD Holiday Calendar 2023  \\n  \\n. \\n Holiday List (G.H.)  \\nHoliday Date  Day \\nNew Year's Day  2 January 202 3 Monday  \\nRepublic Day  26 January 202 3 Thursday  \\nHoli 8 March 202 3 Wednesday  \\nGood Friday/ Tamil New Year/ Mesadi  7 April 202 3 Friday  \\nEid al -Adha  29 June 202 3 Thursday  \\nIndependence Day  15 August 202 3 Tuesday  \\nRaksha Bandhan  30 August 202 3 Wednesday  \\nMahatma Gandhi Jayanti  2 October 202 3 Monday  \\nDussehra/Ayudha Pooja  24 October 202 3 Tuesday  \\nDiwali/Goverdhan Puja  13 November 202 3 Monday  \\nGuru Nanak's Birthday  27 November 202 3 Monday  \\nChristmas  25 December 202 3 Monday  \\nRH 2023 (For Other Dep artments )  \\nRH 2023 (For HRO Dep artment ) \\nHoliday  Date  Day  Holiday  Date  Day \\nMakar Sankranti  13 January 202 3 Friday   \\nMakar Sankranti  13 January 202 3 Friday  \\nPongal/Marten \\nLuther King Day  16 January 202 3 Monday   \\nPongal/Marten \\nLuther King Day  16 January 202 3 Monday  \\nUgadi/Gudi Padwa  22 March 202 3 Wednesday   \\nMaha \\nShivaratri/Shivaratri \\n(HRO only ) 18 February 202 3 Saturday  \\nVaisakhi  14 April 202 3 Friday   \\nUgadi/Gudi Padwa  22 March 202 3 Wednesday  \\nMay Day/ \\nMaharashtra  \\nDay/Gujarat day  01 May 202 3 Monday   \\nVaisakhi  14 April 202 3 Friday  \\nMemorial Day (US)  29 May 202 3 Monday   \\nRamzan Id/Eid -ul-\\nFitar (HRO  only ) 22 April 202 3 Saturday  \\nIndependence Day \\n(US)  04 July 202 3 Tuesday   \\nMemorial Day (US)  29 May 202 3 Monday  \\nOnam  29 August 202 3 Tuesday   \\nIndependence Day \\n(US)  04 July 202 3 Tuesday  \\nLabour Day (US)  04 September 202 3 Monday   \\nMuharram/Ashura  \\n (HRO  only ) 29 July 202 3 Saturday  \\nJanmashtami/Krishn\\na Jayanti  07 September 202 3 Thursday   \\nOnam  29 August 202 3 Tuesday  \\nGanesh \\nChaturthi/Vinayaka \\nChaturthi  19 September 202 3 Tuesday   \\nLabour Day (US)  04 September 202 3 Monday  \\nBhai Duj  15 November 202 3 Wednesday   \\nJanmashtami/Krish\\nna Jayanti  07 September 202 3 Thursday  \\nThanksgiving Day \\n(US)  23 November 202 3 Thursday   \\nGanesh \\nChaturthi/Vinayaka \\nChaturthi  19 September 202 3 Tuesday  \\nThe Friday after \\nThanksgiving Day \\n(US)  24 November 202 3 Friday   \\nBhai Duj  15 November 202 3 Wednesday  \\nChristmas Eve  22 December 202 3 Friday   \\nChristmas Eve  22 December 202 3 Friday\", metadata={'source': 'C:/Users/mpasha1/OneDrive - Insight/Desktop/work/deployopenai/pdf_data/GD Holiday Calendar 2023 (1).pdf', 'page': 0})]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.vectorstores import FAISS\n",
    "from langchain.embeddings import OpenAIEmbeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: Note:  \n",
      "1. The total number of Holidays are restricted to 14 days including 2 RHs (Restricted Holidays) . \n",
      "2. Holidays falling on Saturday will be observed on Friday  and H olidays falling on Sunday will be  observed on Monday.   \n",
      " \n",
      " GD Holiday Calendar 2023  \n",
      "  \n",
      ". \n",
      " Holiday List (G.H.)  \n",
      "Holiday Da\n"
     ]
    }
   ],
   "source": [
    "\n",
    "faiss_index = FAISS.from_documents(pages, embeddings)\n",
    "docs = faiss_index.similarity_search(\"leaves\", k=2)\n",
    "for doc in docs:\n",
    "    print(str(doc.metadata[\"page\"]) + \":\", doc.page_content[:300])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "def parse_response(response):\n",
    "    result = response['result']\n",
    "    source = '\\nSources:'\n",
    "    for source_name in response[\"source_documents\"]:\n",
    "        #print(\"Test :\",source_name)\n",
    "        source_name = re.search(r'[^\\\\/:*?\"<>|\\r\\n]+$', source_name.metadata['source']).group()\n",
    "        # print(source_name)\n",
    "        break\n",
    "    \n",
    "    return result + \"\\n\\n\" + source + source_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import RetrievalQA\n",
    "\n",
    "retriever = faiss_index.as_retriever(include_metadata=True, metadata_key = 'source')\n",
    "\n",
    "chain  = RetrievalQA.from_chain_type(llm=llm, chain_type= \"stuff\", retriever=retriever, return_source_documents=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Holiday Calendar 2023, Holiday List (G.H.), Restricted Holidays (RH), New Year's Day, Republic Day, Holi, Good Friday, Tamil New Year, Mesadi, Eid al-Adha, Independence Day, Raksha Bandhan, Mahatma Gandhi Jayanti, Dussehra, Ayudha Pooja, Diwali, Goverdhan Puja, Guru Nanak's Birthday, Christmas, Makar Sankranti, Pongal, Martin Luther King Day, Ugadi, Gudi Padwa, Maha Shivaratri, Vaisakhi, May Day, Maharashtra Day, Gujarat day, Memorial Day, Ramzan Id, Eid-ul-Fitar, Onam, Labour Day, Muharram, Ashura, Janmashtami, Krishna Jayanti, Ganesh Chaturthi, Vinayaka Chaturthi, Bhai Duj, Thanksgiving Day, Christmas Eve.\n",
      "\n",
      "\n",
      "Sources:GD Holiday Calendar 2023 (1).pdf\n"
     ]
    }
   ],
   "source": [
    "query = \"provid me key phrases of all data our document provided ?\"\n",
    "\n",
    "response = chain(query)\n",
    "\n",
    "print(parse_response(response))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
